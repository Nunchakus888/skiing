#!/usr/bin/env python3
"""
Smart Watermark Remover V6 - Multi-Threshold Enhanced
Strategy: Grounded-SAM + Multi-Scale Detection + Human Protection

Optimization Features:
1. å¤šé˜ˆå€¼èåˆæ£€æµ‹ (0.30/0.20/0.15) - æ•è·æ¨¡ç³Š/åŠé€æ˜æ°´å°
2. æ”¹è¿› Prompt (ä¸­è‹±æ–‡ + å…·ä½“æè¿°) - æå‡è¯­ä¹‰ç†è§£
3. å¢å¼ºå½¢æ€å­¦å¤„ç† (iterations=3) - è¦†ç›–è¾¹ç¼˜æ®‹ç•™
4. äººç‰©ä¿æŠ¤æœºåˆ¶ - é¿å…è¯¯ä¼¤ä¸»ä½“

Further Optimization Options:
- å‡çº§ SAM_TYPE ä¸º "vit_l" æˆ– "vit_h" (æ›´ç²¾ç»†åˆ†å‰²)
- é™ä½é˜ˆå€¼åˆ° 0.05 (æé™æ¨¡å¼ï¼Œä½†å¯èƒ½è¯¯æ£€)
- æ·»åŠ å›¾åƒå¢å¼ºé¢„å¤„ç† (CLAHE)
"""

import os
import sys
import cv2
import torch
import numpy as np
import datetime
from PIL import Image
from diffusers import StableDiffusionInpaintPipeline
from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
from segment_anything import sam_model_registry, SamPredictor

# Configuration
GDINO_MODEL = "IDEA-Research/grounding-dino-base"

# SAM Model Selection (å‡çº§åˆ°æ›´å¤§æ¨¡å‹ä»¥æé«˜ç²¾åº¦)
# vit_b: 90M params, 375MB  (å½“å‰)
# vit_l: 300M params, 1.2GB (æ¨è)
# vit_h: 600M params, 2.4GB (æœ€å¼º)
SAM_TYPE = "vit_b"  # å¯æ”¹ä¸º "vit_l" æˆ– "vit_h"
SAM_URLS = {
    "vit_b": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth",
    "vit_l": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth",
    "vit_h": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
}
SAM_CHECKPOINT = os.path.expanduser(f"~/.cache/sam_{SAM_TYPE}.pth")
SAM_URL = SAM_URLS[SAM_TYPE]

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

class SmartRemover:
    def __init__(self):
        self.device = self._get_device()
        print(f"ğŸš€ åˆå§‹åŒ–äººç‰©ä¿æŠ¤å‹å»æ°´å°ç³»ç»Ÿ (Device: {self.device})")
        
        # 1. Load GroundingDINO
        print("   [1/3] åŠ è½½ GroundingDINO (è§†è§‰å®šä½)...")
        try:
            self.processor = AutoProcessor.from_pretrained(GDINO_MODEL)
            self.detector = AutoModelForZeroShotObjectDetection.from_pretrained(GDINO_MODEL).to(self.device)
        except Exception as e:
            print(f"âŒ GroundingDINO åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)

        # 2. Load SAM
        print("   [2/3] åŠ è½½ SAM (ç²¾ç»†åˆ†å‰²)...")
        self._ensure_sam_model()
        try:
            import functools
            original_load = torch.load
            torch.load = functools.partial(original_load, weights_only=False)
            sam = sam_model_registry[SAM_TYPE](checkpoint=SAM_CHECKPOINT)
            torch.load = original_load
            sam.to(self.device)
            self.sam_predictor = SamPredictor(sam)
        except Exception as e:
            print(f"âŒ SAM åŠ è½½å¤±è´¥: {e}\n   rm {SAM_CHECKPOINT}")
            sys.exit(1)

        # 3. Load SD Inpainting
        print("   [3/3] åŠ è½½å›¾åƒä¿®å¤æ¨¡å‹...")
        self.dtype = torch.float16 if self.device == "cuda" else torch.float32
        self.pipe = StableDiffusionInpaintPipeline.from_pretrained(
            "runwayml/stable-diffusion-inpainting",
            torch_dtype=self.dtype,
            safety_checker=None
        ).to(self.device)
        if self.device == "cuda": self.pipe.enable_attention_slicing()
        print("âœ“ ç³»ç»Ÿå°±ç»ª\n")

    def _get_device(self):
        if torch.cuda.is_available(): return "cuda"
        if torch.backends.mps.is_available(): return "mps"
        return "cpu"

    def _ensure_sam_model(self):
        if os.path.exists(SAM_CHECKPOINT):
            # Header check (PK..)
            with open(SAM_CHECKPOINT, "rb") as f: header = f.read(4)
            if os.path.getsize(SAM_CHECKPOINT) > 100*1024*1024 and (header.startswith(b'PK') or header.startswith(b'\x80')):
                return
            os.remove(SAM_CHECKPOINT)
        
        print(f"   ğŸ“¥ ä¸‹è½½ SAM æ¨¡å‹...")
        os.makedirs(os.path.dirname(SAM_CHECKPOINT), exist_ok=True)
        os.system(f"curl -L -k -o {SAM_CHECKPOINT} {SAM_URL}")
        if not os.path.exists(SAM_CHECKPOINT):
            os.system(f"wget --no-check-certificate {SAM_URL} -O {SAM_CHECKPOINT}")

    def detect(self, image_pil, prompt, box_threshold=0.3):
        """Generic detection wrapper"""
        inputs = self.processor(images=image_pil, text=prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.detector(**inputs)
        
        target_sizes = torch.tensor([image_pil.size[::-1]], device=self.device)
        results = self.processor.image_processor.post_process_object_detection(
            outputs, threshold=box_threshold, target_sizes=target_sizes
        )[0]
        return results["boxes"].cpu().numpy(), results["scores"].cpu().numpy()

    def get_sam_mask(self, image_cv, boxes):
        """Get binary mask from boxes using SAM"""
        if len(boxes) == 0: return np.zeros(image_cv.shape[:2], dtype=np.uint8)
        
        transformed_boxes = self.sam_predictor.transform.apply_boxes_torch(
            torch.tensor(boxes, device=self.device), image_cv.shape[:2]
        )
        masks, _, _ = self.sam_predictor.predict_torch(
            point_coords=None, point_labels=None,
            boxes=transformed_boxes, multimask_output=False,
        )
        # Combine all masks
        if len(masks) == 0: return np.zeros(image_cv.shape[:2], dtype=np.uint8)
        
        # masks: (N, 1, H, W) -> (N, H, W) -> (H, W)
        final = torch.any(masks.squeeze(1), dim=0).cpu().numpy().astype(np.uint8) * 255
        return np.ascontiguousarray(final)

    def detect_multiscale_enhanced(self, image_pil, image_cv, prompt):
        """
        å¢å¼ºç‰ˆå¤šå°ºåº¦æ£€æµ‹:
        1. å¤šé˜ˆå€¼ (0.25, 0.15, 0.08)
        2. å¤šå°ºåº¦å›¾åƒ (1x, 1.5x, 2x) - æ”¾å¤§åå°æ°´å°æ›´æ˜“æ£€æµ‹
        3. å›¾åƒå¢å¼º (CLAHE)
        """
        all_boxes, all_scores = [], []
        orig_w, orig_h = image_pil.size
        
        # ç­–ç•¥1: åŸå›¾å¤šé˜ˆå€¼ (æä½é˜ˆå€¼æ•è·æ¨¡ç³Šæ°´å°)
        for thresh in [0.25, 0.15, 0.08, 0.05]:
            boxes, scores = self.detect(image_pil, prompt, thresh)
            all_boxes.extend(boxes)
            all_scores.extend(scores)
        
        # ç­–ç•¥2: æ”¾å¤§å›¾åƒæ£€æµ‹ï¼ˆæ•è·å°æ°´å°ï¼‰
        for scale in [1.5, 2.0, 2.5]:  # å¢åŠ  2.5x è¶…çº§æ”¾å¤§
            scaled_w = int(orig_w * scale)
            scaled_h = int(orig_h * scale)
            scaled_img = image_pil.resize((scaled_w, scaled_h), Image.BICUBIC)
            
            boxes, scores = self.detect(scaled_img, prompt, 0.10)  # é™ä½é˜ˆå€¼
            if len(boxes) > 0:
                # æ˜ å°„å›åŸå›¾åæ ‡
                boxes_scaled = boxes / scale
                all_boxes.extend(boxes_scaled)
                all_scores.extend(scores)
        
        # ç­–ç•¥3: CLAHE å¢å¼ºåæ£€æµ‹ï¼ˆä½å¯¹æ¯”åº¦æ°´å°ï¼‰
        lab = cv2.cvtColor(image_cv, cv2.COLOR_BGR2LAB)
        lab[:, :, 0] = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8)).apply(lab[:, :, 0])
        enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        enhanced_pil = Image.fromarray(cv2.cvtColor(enhanced, cv2.COLOR_BGR2RGB))
        
        # å¤šé˜ˆå€¼æ£€æµ‹å¢å¼ºå›¾
        for thresh in [0.15, 0.08, 0.05]:
            boxes, scores = self.detect(enhanced_pil, prompt, thresh)
            all_boxes.extend(boxes)
            all_scores.extend(scores)
        
        if len(all_boxes) == 0:
            return np.array([]), np.array([])
        
        # NMS å»é‡
        boxes_np = np.array(all_boxes)
        scores_np = np.array(all_scores)
        keep = []
        indices = np.argsort(scores_np)[::-1]
        
        while len(indices) > 0:
            i = indices[0]
            keep.append(i)
            if len(indices) == 1: break
            
            box1 = boxes_np[i]
            rest_boxes = boxes_np[indices[1:]]
            
            x1 = np.maximum(box1[0], rest_boxes[:, 0])
            y1 = np.maximum(box1[1], rest_boxes[:, 1])
            x2 = np.minimum(box1[2], rest_boxes[:, 2])
            y2 = np.minimum(box1[3], rest_boxes[:, 3])
            
            inter = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)
            area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
            area2 = (rest_boxes[:, 2] - rest_boxes[:, 0]) * (rest_boxes[:, 3] - rest_boxes[:, 1])
            iou = inter / (area1 + area2 - inter + 1e-6)
            
            indices = indices[1:][iou < 0.5]
        
        return boxes_np[keep], scores_np[keep]
    
    def detect_edge_fallback(self, image_cv):
        """è¾¹ç¼˜æ£€æµ‹å›é€€æ–¹æ¡ˆ - æ•è·é—æ¼çš„é•¿æ¡å½¢æ°´å°"""
        gray = cv2.cvtColor(image_cv, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 80, 200)
        
        # æ°´å¹³æ–¹å‘é—­è¿ç®—ï¼ˆè¿æ¥æ°´å°å­—ç¬¦ï¼‰
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 3))
        closed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)
        
        contours, _ = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        h, w = image_cv.shape[:2]
        boxes = []
        for cnt in contours:
            x, y, cw, ch = cv2.boundingRect(cnt)
            aspect = cw / max(ch, 1)
            area = cw * ch
            # æ°´å°ç‰¹å¾: é•¿æ¡å½¢ (aspect > 3), é¢ç§¯é€‚ä¸­
            if aspect > 3 and 1000 < area < h * w * 0.03:
                boxes.append([x, y, x + cw, y + ch])
        
        return np.array(boxes) if boxes else np.array([])

    def generate_mask(self, image_path):
        print(f"ğŸ§  åˆ†æå›¾åƒ: {os.path.basename(image_path)}")
        image_pil = Image.open(image_path).convert("RGB")
        image_cv = cv2.imread(image_path)
        self.sam_predictor.set_image(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB))
        h, w = image_cv.shape[:2]

        # 1. å¢å¼ºå¤šå°ºåº¦æ£€æµ‹
        print("   [1/4] æ‰«ææ°´å° (å¤šå°ºåº¦å¢å¼º)...")
        wm_prompt = "text overlay. watermark. translucent text. copyright. timestamp. username. æ°´å°. æ–‡å­—."
        wm_boxes, wm_scores = self.detect_multiscale_enhanced(image_pil, image_cv, wm_prompt)
        
        print(f"       GroundingDINO: {len(wm_boxes)} å¤„")
        
        # 2. å›é€€æœºåˆ¶ï¼šè¾¹ç¼˜æ£€æµ‹
        if len(wm_boxes) < 5:
            print("   [1.5/4] å¯ç”¨è¾¹ç¼˜æ£€æµ‹è¾…åŠ©...")
            edge_boxes = self.detect_edge_fallback(image_cv)
            if len(edge_boxes) > 0:
                print(f"       è¾¹ç¼˜æ£€æµ‹: +{len(edge_boxes)} å¤„")
                wm_boxes = np.vstack([wm_boxes, edge_boxes]) if len(wm_boxes) > 0 else edge_boxes
                wm_scores = np.concatenate([wm_scores, np.ones(len(edge_boxes)) * 0.5]) if len(wm_scores) > 0 else np.ones(len(edge_boxes)) * 0.5
        
        if len(wm_boxes) == 0:
            print("   âš ï¸ æœªå‘ç°æ°´å°")
            return np.zeros((h, w), dtype=np.uint8)
        
        print(f"       æ€»è®¡æ£€æµ‹: {len(wm_boxes)} å¤„æ°´å°")

        # 2. æ‰«æäººç‰© (ä¿æŠ¤)
        print("   [2/4] æ‰«æäººç‰© (ä¿æŠ¤)...")
        p_boxes, p_scores = self.detect(image_pil, "person.", 0.40)

        # 3. SAM ç²¾ç»†åˆ†å‰²
        print("   [3/4] SAM ç²¾ç»†åˆ†å‰²...")
        wm_mask = self.get_sam_mask(image_cv, wm_boxes)
        
        if len(p_boxes) > 0:
            p_mask = self.get_sam_mask(image_cv, p_boxes)
            print(f"   [4/4] æ™ºèƒ½èåˆ (æ°´å°:{len(wm_boxes)} - äººç‰©:{len(p_boxes)})...")
            
            # äººç‰©ä¿æŠ¤åŒºè†¨èƒ€
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20, 20))
            p_mask_safe = cv2.dilate(p_mask, kernel, iterations=3)
            
            # é€»è¾‘è¿ç®—: Watermark - Person
            final_mask = cv2.bitwise_and(wm_mask, cv2.bitwise_not(p_mask_safe))
        else:
            print(f"   [4/4] æœªæ£€æµ‹åˆ°äººç‰©ï¼Œç›´æ¥ç”Ÿæˆ Mask...")
            final_mask = wm_mask
        
        # åå¤„ç†ï¼šè¿é€šåŸŸåˆå¹¶ + å¼ºåŠ›è†¨èƒ€è¦†ç›–æ®‹ç•™
        # åˆå¹¶é™„è¿‘çš„å°å—ï¼ˆæ°´å°å¾€å¾€æ˜¯åˆ†æ•£çš„å¤šä¸ªå­—ç¬¦ï¼‰
        kernel_merge = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 7))
        final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_CLOSE, kernel_merge)
        
        # å¼ºåŠ›è†¨èƒ€è¦†ç›–è¾¹ç¼˜æ®‹ç•™ (å½»åº•æ¶ˆé™¤æ–‡å­—è¾¹ç¼˜)
        kernel_dilate = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 7))
        final_mask = cv2.dilate(final_mask, kernel_dilate, iterations=6)

        coverage = np.sum(final_mask > 0) / (h * w)
        print(f"   âœ“ Maskå®Œæˆ (è¦†ç›–ç‡: {coverage:.1%})")
        
        return final_mask

    def process(self, image_path):
        base, ext = os.path.splitext(image_path)
        hms = datetime.datetime.now().strftime("%m-%d%H%M%S")
        output_path = f"{base}_{hms}{ext}"
        
        # 1. Mask
        mask_np = self.generate_mask(image_path)
        if np.sum(mask_np) == 0: return None
        
        cv2.imwrite(f"{base}_mask.png", mask_np)
        
        # 2. Prepare
        image_cv = cv2.imread(image_path)
        orig_h, orig_w = image_cv.shape[:2]
        
        # Resize for SD
        w, h = (orig_w // 8) * 8, (orig_h // 8) * 8
        
        # 3. Two-Stage Inpainting (æ··åˆç­–ç•¥)
        print(f"ğŸ¨ ä¿®å¤ä¸­ ({w}x{h})...")
        
        # Stage 1: OpenCV Inpaint (ä¿å®ˆçº¹ç†å¡«å……ï¼Œç»ä¸æ·»åŠ æ–°å…ƒç´ )
        print("   [Stage 1/2] OpenCV çº¹ç†å¡«å……...")
        image_cv_resized = cv2.resize(image_cv, (w, h), interpolation=cv2.INTER_LANCZOS4)
        mask_cv_resized = cv2.resize(mask_np, (w, h), interpolation=cv2.INTER_NEAREST)
        
        # Telea ç®—æ³•ï¼šåŸºäºå¿«é€Ÿè¡Œè¿›æ³•ï¼Œçº¯ç²¹å¤åˆ¶å‘¨å›´çº¹ç†
        opencv_result = cv2.inpaint(image_cv_resized, mask_cv_resized, 5, cv2.INPAINT_TELEA)
        opencv_result_pil = Image.fromarray(cv2.cvtColor(opencv_result, cv2.COLOR_BGR2RGB))
        
        # Stage 2: SD è½»åº¦ Refine (ä»…å¹³æ»‘è¾¹ç¼˜ï¼Œä¸æ”¹å˜å†…å®¹)
        print("   [Stage 2/2] SD è¾¹ç¼˜å¹³æ»‘...")
        mask_pil = Image.fromarray(mask_cv_resized).convert("L")
        
        result = self.pipe(
            # æç®€ promptï¼šåªå¹³æ»‘ï¼Œä¸ç”Ÿæˆ
            prompt="smooth edges, blend seamlessly, no changes",
            # è¶…å¼º negativeï¼šç¦æ­¢ä¸€åˆ‡ç”Ÿæˆè¡Œä¸º
            negative_prompt=(
                "new content, generated content, created content, synthetic content, "
                "person, people, human, face, body, character, figure, skier, "
                "man, woman, child, athlete, "
                "object, objects, element, elements, "
                "watermark, text, logo, letters, "
                "change, modification, alteration, addition, "
                "distorted, artifacts, blurry"
            ),
            image=opencv_result_pil,  # âš ï¸ è¾“å…¥æ˜¯ OpenCV ç»“æœï¼Œä¸æ˜¯åŸå›¾
            mask_image=mask_pil,
            num_inference_steps=30,   # æå°‘æ­¥æ•°ï¼Œåªå¹³æ»‘è¾¹ç¼˜
            guidance_scale=15.0,      # æœ€å¼ºå¼•å¯¼
            strength=0.35             # â­â­â­ æä½ï¼šåªå…è®¸ 35% ä¿®æ”¹ï¼ˆä¸»è¦ç”¨äºå¹³æ»‘ï¼‰
        ).images[0]
        
        result = result.resize((orig_w, orig_h), Image.LANCZOS)
            
        # Composite
        orig_img = Image.open(image_path).convert("RGB")
        final = Image.composite(result, orig_img, Image.fromarray(mask_np).convert("L"))
        
        final.save(output_path, quality=95)
        print(f"âœ¨ å®Œæˆ: {output_path}")
        return output_path

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python remove_watermark_smart_v2.py <img_path>")
    else:
        if os.path.exists(sys.argv[1]):
            SmartRemover().process(sys.argv[1])
        else:
            print("File not found")
